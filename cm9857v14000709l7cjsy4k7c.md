---
title: "ðŸ§  From Zero to Hero : Mastering Recurrent Neural Networks (RNNs)"
datePublished: Tue Apr 08 2025 06:52:25 GMT+0000 (Coordinated Universal Time)
cuid: cm9857v14000709l7cjsy4k7c
slug: nlp-rnn
cover: https://cdn.hashnode.com/res/hashnode/image/stock/unsplash/ywqa9IZB-dU/upload/7618b5dd8c91731d5925ecb7edac2ca2.jpeg
tags: nlp, basics, rnn

---

let's dive into the wonderful world of **Recurrent Neural Networks (RNNs)** â€” from the **ground up** to becoming a **hero** with a deep understanding of how they work, the challenges they face, and how to use them in real-world applications, especially in **natural language processing (NLP)** and **time series prediction**.

---

## ðŸŽ¯ Introduction: Predicting the Future with Neural Memory

Imagine this: you're playing cricket. The batter strikes the ball, and instantly, your body *predicts* its trajectory. You sprint, adjust, leap, and catch. All this in a matter of seconds.

This **ability to predict what comes next** â€” whether itâ€™s a ball, a word, or a stock price â€” is a skill we take for granted. And this is exactly what **Recurrent Neural Networks (RNNs)** try to mimic in machines.

---

## ðŸ§© Why Do We Need RNNs?

Unlike feedforward neural networks that process **fixed-sized inputs and outputs**, RNNs are **designed for sequences**. Theyâ€™re capable of:

* Analyzing **time-series** data like weather or stock prices.
    
* Generating **music**, like Googleâ€™s Magenta project.
    
* Translating languages (e.g., English â†’ French).
    
* Capturing the **context** in language for chatbots and assistants.
    
* Predicting the next word in a sentence or captioning an image.
    

---

## ðŸ” What Makes RNNs Special?

The magic lies in their **feedback loop**: at each time step, an RNN **remembers** what it saw previously.

## ðŸ”„ Recurrent Neurons: Learning From the Past

Most neural networks are feedforwardâ€”they process input in one direction: from input to output.

RNNs, however, introduce **feedback loops**. A basic recurrent neuron not only receives an input x(t) at time step t, but also its **own output** from the previous time step y(tâˆ’1). This loop allows it to **remember** past information.

**Unrolling through time** shows how this memory works. At each time step, the neuron processes both the current input and its previous state. Hereâ€™s the idea:

`Input â†’ Output â†’ Feeds back â†’ Used in next time step`

When we create **layers** of such neurons, each neuron receives:

* The current input vector x(t)
    
* The previous output vector y(tâˆ’1)
    

This results in the network developing an evolving **internal state**, capturing temporal dependencies in the data

---

### ðŸ§  The Math Behind the Memory

Each recurrent neuron has two weight matrices:

* Wxâ€‹: Weights for the current input
    
* Wyâ€‹: Weights for the previous output
    

The output at time t is calculated as:

$$y(t)=Ï•(Wxâ€‹â‹…x(t)+Wyâ€‹â‹…y(tâˆ’1)+b)$$

Where:

* Ï• is an activation function (like ReLU or tanh)
    
* b is the bias term
    

This structure enables **gradient-based learning** over sequences, where the state at each time depends on all previous inputs since t=0.

---

## ðŸ”“ Unrolling RNNs Through Time

To understand an RNN, imagine **unrolling it across time** â€” as if you stretch the loop into a straight line. This shows how each time step connects to the previous one, forming a **chain-like architecture**.

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1744094119136/eb67da4b-9295-4fb0-8c51-966e860714ec.png align="center")

!\[\](file:///home/abhishek/Pictures/Screenshots/Screenshot%20from%202025-04-08%2011-04-09.png?msec=1744090506188 align="left")

At each step:

* The input enters
    
* The previous state is reused
    
* A new state is calculated
    

---

## ðŸ§  Memory Cells and Hidden States

The state carries information forward. This is why we call these networks **recurrent** â€” they maintain a **memory** of past inputs.

> A "memory cell" is a part of the network that retains information over time.

---

## ðŸ—ï¸ RNN Architectures: Types of Input/Output Sequences

RNNs are **flexible** with input and output lengths:

| Type | Description | Use Case |
| --- | --- | --- |
| **Many-to-Many** | Sequence in â†’ Sequence out | Translation, video tagging |
| **Many-to-One** | Sequence in â†’ Single output | Sentiment analysis |
| **One-to-Many** | One input â†’ Sequence out | Image captioning |
| **Encoder-Decoder** | Sequence in â†’ Vector â†’ Sequence out | Machine Translation |

---

# ðŸ§  Zero to Hero: Understanding RNNs in TensorFlow (With Code)

Let's write a **comprehensive â€œZero to Heroâ€ blog post** on building **Basic RNNs with TensorFlow**. This will include:

* Manual unrolling (from scratch)
    
* Using `static_rnn`
    
* Using `dynamic_rnn`
    
* Handling variable-length sequences
    

---

Recurrent Neural Networks (RNNs) are the backbone of sequential modeling tasks like time-series prediction, natural language processing, and more. This blog is your step-by-step guide to implementing RNNs in TensorFlow, starting from scratch and progressively moving toward more advanced and practical implementations.

---

## ðŸ› ï¸ Part 1: Building an RNN From Scratch

Letâ€™s understand the **core mechanism** behind RNNs by manually unrolling it for two time steps.

### ðŸ”§ Manual RNN (Unrolled for 2 Steps)

**What we did:**  
We manually defined the RNN's operation step-by-step using matrix multiplication and `tanh` activation.

**Why:**  
To understand **how RNNs work internally**. Every RNN cell:

* Takes current input Xt
    
    * Takes the previous hidden state h(tâˆ’1)â€‹
        
* Produces a new hidden state ht â€‹= tanh(Wxâ‹…xtâ€‹+Whâ‹…h(tâˆ’1)â€‹+b)
    

This gives intuition about **weight sharing**, temporal processing, and recursive computation.

```python
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
import numpy as np

# Hyperparameters
n_inputs = 3
n_neurons = 5

# Input placeholders for two time steps
X0 = tf.placeholder(tf.float32, [None, n_inputs])
X1 = tf.placeholder(tf.float32, [None, n_inputs])

# Weights & biases (shared across time steps)
Wx = tf.Variable(tf.random_normal(shape=[n_inputs, n_neurons]))
Wy = tf.Variable(tf.random_normal(shape=[n_neurons, n_neurons]))
b = tf.Variable(tf.zeros([1, n_neurons]))

# Unrolling manually
Y0 = tf.tanh(tf.matmul(X0, Wx) + b)
Y1 = tf.tanh(tf.matmul(X1, Wx) + tf.matmul(Y0, Wy) + b)

# Session
init = tf.global_variables_initializer()
X0_batch = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 0, 1]])
X1_batch = np.array([[9, 8, 7], [0, 0, 0], [6, 5, 4], [3, 2, 1]])

with tf.Session() as sess:
    init.run()
    Y0_val, Y1_val = sess.run([Y0, Y1], feed_dict={X0: X0_batch, X1: X1_batch})
    print("Y0 (t=0):\n", Y0_val)
    print("\nY1 (t=1):\n", Y1_val)
```

ðŸŽ¯ This shows how **shared weights** and **recurrent computation** define the RNN structure.

---

## ðŸ” Part 2: Static Unrolling with `static_rnn()`

**What we did:**  
We replaced our manual math with TensorFlowâ€™s RNNCell API.

**Why:**  
To simplify RNN construction while still explicitly unrolling each time step. It's good for **educational purposes** or when working with **short, fixed-length sequences**.

```python
X0 = tf.placeholder(tf.float32, [None, n_inputs])
X1 = tf.placeholder(tf.float32, [None, n_inputs])

# Define cell
basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)

# Chain the cells manually
output_seqs, states = tf.contrib.rnn.static_rnn(
    basic_cell, [X0, X1], dtype=tf.float32
)
Y0, Y1 = output_seqs
```

âž¡ï¸ Still manually unrolled, but now using RNN cells instead of raw matrix operations.

---

## ðŸ§± Part 3: Feeding Whole Sequences

**What we did:**  
Passed the entire sequence (e.g., a sentence or time series) in a single placeholder.

**Why:**  
This makes input handling more scalable and prepares us for **real data** â€” like batches of sequences with multiple time steps. This also matches how data is structured in NLP and other sequence modeling tasks.

```python
n_steps = 2
X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])

# Prepare input for static_rnn
X_seqs = tf.unstack(tf.transpose(X, perm=[1, 0, 2]))  # time-major

basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)
output_seqs, states = tf.contrib.rnn.static_rnn(basic_cell, X_seqs, dtype=tf.float32)

outputs = tf.transpose(tf.stack(output_seqs), perm=[1, 0, 2])  # batch-major

# Example batch
X_batch = np.array([
    [[0, 1, 2], [9, 8, 7]],
    [[3, 4, 5], [0, 0, 0]],
    [[6, 7, 8], [6, 5, 4]],
    [[9, 0, 1], [3, 2, 1]]
])

with tf.Session() as sess:
    init.run()
    outputs_val = outputs.eval(feed_dict={X: X_batch})
    print(outputs_val)
```

âœ… Now, everythingâ€™s easier to manage â€” even if you have **many time steps**.

---

## âš¡ Part 4: Dynamic Unrolling with `dynamic_rnn()`

**What we did:**  
Used TensorFlowâ€™s `dynamic_rnn()` to handle sequence processing internally.

**Why:**  
Dynamic RNN:

* Automatically unrolls for the number of time steps
    
* Builds a **smaller, more efficient graph**
    
* Handles **longer sequences** better
    
* Avoids redundant code
    

This is the preferred method in production or real model training.

```python
X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])

basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)
outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)

with tf.Session() as sess:
    init.run()
    outputs_val, states_val = sess.run([outputs, states], feed_dict={X: X_batch})
    print(outputs_val)
```

ðŸ’¡ Bonus: You can enable `swap_memory=True` to manage GPU memory better during backpropagation.

---

## ðŸ“ Part 5: Handling Variable-Length Sequences

**What we did:**  
Used `sequence_length` to inform TensorFlow how many steps are **valid** for each sequence in a batch.

**Why:**  
In real-world NLP or speech tasks:

* Sequences are **padded** to make them the same length
    
* But you donâ€™t want the RNN to **learn from padding**
    

Using `sequence_length` tells the RNN to:

* Stop computing after the actual data
    
* Ignore padded parts during backpropagation
    

```python
seq_length = tf.placeholder(tf.int32, [None])

outputs, states = tf.nn.dynamic_rnn(
    basic_cell, X, dtype=tf.float32, sequence_length=seq_length
)

# Variable-length batch
X_batch = np.array([
    [[0, 1, 2], [9, 8, 7]],
    [[3, 4, 5], [0, 0, 0]],  # padded
    [[6, 7, 8], [6, 5, 4]],
    [[9, 0, 1], [3, 2, 1]]
])
seq_length_batch = np.array([2, 1, 2, 2])  # second instance ends at step 1

with tf.Session() as sess:
    init.run()
    outputs_val, states_val = sess.run(
        [outputs, states],
        feed_dict={X: X_batch, seq_length: seq_length_batch}
    )
    print(outputs_val)
    print(states_val)
```

ðŸ’¬ After `sequence_length` is set:

* Outputs after the valid length are **zero vectors**
    
* Final states ignore padded steps
    
    ---
    
    ## ðŸ§—â€â™‚ï¸ Challenges of Long Sequences
    
    RNNs work wellâ€”but training over long sequences opens two cans of worms:
    
    ### 1\. **Vanishing/Exploding Gradients**
    
    Training becomes unstable as gradients vanish or explode over many time steps.
    
    ðŸ”§ **Solutions**:
    
    * ReLU or Leaky ReLU activations
        
    * Batch Normalization
        
    * Good weight initialization
        
    * Gradient clipping
        
    * Optimizers like RMSProp or Adam
        
    
    ### 2\. **Memory Fade**
    
    As the sequence gets longer, earlier inputs vanish in memory. Imagine a sentiment analysis model forgetting the opening "I loved this movie" by the end of a 500-word review!
    
    ### ðŸ§  Enter: LSTM and GRU Cells
    
    To fix fading memory, we need smarter cells with long-term memory. Thatâ€™s where **LSTM** (Long Short-Term Memory) and **GRU** (Gated Recurrent Unit) come into play. We'll deep dive into these in the next postâ€”but hereâ€™s a teaser: they *remember what matters and forget what doesnâ€™t.*
    
    ---
    
    ## â³ Truncated Backpropagation Through Time (TBPTT)
    
    Instead of unrolling RNNs over 100s of steps, why not train on **shorter sub-sequences**?
    
    âœ… Pros: Faster training, avoids extreme vanishing gradients  
    â˜ ï¸ Cons: Can't capture very long-term dependencies
    
    ---
    
    ## ðŸ§  Summary: Deep RNNs in the Real World
    
    | Problem | Solution |
    | --- | --- |
    | Modeling long-term dependencies | Stack RNN layers |
    | Slow training on long sequences | Truncated BPTT |
    | Forgetting early info | Use LSTM/GRU |
    | Overfitting | Apply dropout |
    | GPU bottlenecks | Distribute RNN layers across GPUs |
    

> ðŸ‘£ **Next step?** Dive into **LSTM cells**, **GRUs**, and eventually explore **attention mechanisms**â€”theyâ€™re the secret sauce behind modern sequence models like Transformers!

---