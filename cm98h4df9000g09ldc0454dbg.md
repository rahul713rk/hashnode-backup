---
title: "ğŸ” Zero to Hero: Tokenization in NLP â€“ From Basics to Subword Models"
datePublished: Tue Apr 08 2025 12:25:38 GMT+0000 (Coordinated Universal Time)
cuid: cm98h4df9000g09ldc0454dbg
slug: zero-to-hero-tokenization-in-nlp-from-basics-to-subword-models
cover: https://cdn.hashnode.com/res/hashnode/image/upload/v1744114179103/e4016abd-665e-4a92-9c7b-66591f291b33.webp
tags: nlp, deep-learning, token, tokenization, bpe, wordpiece, sentencepiece

---

### âœ¨ â€œHow can machines read language?â€

That's the fundamental question behind **tokenization**, the process of breaking down raw text into manageable, machine-readable pieces. If you're on your journey to becoming a data scientist or working with Natural Language Processing (NLP), tokenization is one of the most critical concepts youâ€™ll master.

In this blog, weâ€™ll walk from the **ground level of tokenization** to the **heights of modern subword algorithms** like **Byte Pair Encoding (BPE)**, **WordPiece**, and **SentencePiece**.

---

## ğŸ“˜ What is Tokenization?

Tokenization is the process of converting raw text into smaller units called **tokens**. These can be words, characters, or subwords. Once tokenized, we convert these tokens into numbers (indices or embeddings) for processing by a neural network.

Letâ€™s look at a naive tokenization example:

```python
text = open('example.txt', 'r').read()
words = text.split(" ")
tokens = {v: k for k, v in enumerate(words)}
```

This simply maps each word to an index. While this is straightforward, it's quite limitedâ€”it doesnâ€™t account for punctuation, inflections, or compound words. We need more sophisticated techniques to truly empower machines to "read".

---

## ğŸ¤” Why Do We Need Tokenization?

The question isnâ€™t just *how* to make machines read, but *how to make them understand*. Raw text is not useful until we break it into comprehensible pieces.

* **Humans** understand language through sound, meaning, context.
    
* **Machines** donâ€™t. They only understand **tokens**, which are then encoded into vectors.
    

A good tokenizer allows a model to:

* Handle **infrequent and compound words**
    
* Work across **multiple languages**, even those with no clear word boundaries (like Chinese)
    
* **Generalize** well beyond its training vocabulary
    

---

## ğŸ§± Types of Tokenization

Letâ€™s explore several approaches to tokenization, each with its pros and cons.

### 1ï¸âƒ£ Word-Based Tokenization

This is the classic wayâ€”just split the text on whitespace.

```python
"let's go home" â†’ ["let's", "go", "home"]
```

#### âš ï¸ Problems:

* Fails to generalize to unseen words (`football` â‰  `foot` + `ball`)
    
* Huge vocabulary requirement
    
* Can't handle slangs, compound words, or languages without spaces
    

### 2ï¸âƒ£ Character-Based Tokenization

Instead of words, break the text into **characters**:

```python
"hello" â†’ ["h", "e", "l", "l", "o"]
```

#### âœ… Pros:

* Handles unseen words effortlessly
    
* Language-independent
    

#### âŒ Cons:

* Very long input sequences
    
* Higher compute cost
    
* No inherent semantics (semantics arise only after extensive learning)
    

---

## ğŸ§© Subword Tokenization: The Best of Both Worlds

Subword tokenization is a middle groundâ€”it breaks words into **meaningful units**, like prefixes, suffixes, or roots.

> **Example**:  
> `"unhappily"` â†’ \["un", "happ", "ily"\]

This way, even unseen words can be processed if the model knows their **subparts**.

---

## ğŸ” Byte Pair Encoding (BPE)

Originally a data compression algorithm, **BPE** is now a popular subword tokenization technique.

### ğŸ”§ How BPE Works:

1. Add a word-end marker (`</w>`) to each word.
    
2. Split all words into **characters**.
    
3. Count all **adjacent character pairs**.
    
4. Merge the most frequent pair.
    
5. Repeat until you hit a limit (iterations or vocabulary size).
    

### ğŸ§ª BPE Example:

Letâ€™s say we have:

```python
"There is an 80% chance of rainfall today. We are pretty sure it is going to rain."
```

#### Step-by-step:

* Words â†’ characters: `"rain"` â†’ `["r", "a", "i", "n", "</w>"]`
    
* Count pairs: ("r", "a"), ("a", "i"), etc.
    
* Merge the most frequent ones: maybe ("r", "a") â†’ "ra"
    
* Repeat...
    

Eventually:

```markdown
"rainfall" â†’ ["rain", "fall"]
"unhappily" â†’ ["un", "happ", "ily"]
```

#### ğŸ§  Pros:

* Efficient
    
* Reduces tokens
    
* Vocabulary is controllable
    

#### ğŸ˜µâ€ğŸ’« Cons:

* Greedy algorithm (might not find global optima)
    
* Results vary based on iteration count
    
* Deterministic, no randomness in sampling
    

---

## ğŸ”¡ WordPiece: Subword Regularization with Probability

Developed by Google for **BERT**, WordPiece builds on BPE but with a twistâ€”it chooses merges based on how much they **increase likelihood** of training data, not just raw frequency.

### ğŸ“Œ How it works:

* Uses a **language model objective** to decide merges
    
* Tokens include special markers like `##` to indicate subwords
    

> **Example**:  
> `"unhappily"` â†’ `["un", "##happi", "##ly"]`

### âœ… Benefits:

* Better handling of rare/unknown words
    
* More robust than BPE
    
* Language-specific patterns can emerge
    

---

## ğŸ§  SentencePiece: Tokenization Without Spaces

Developed by Google (again), **SentencePiece** is differentâ€”it doesnâ€™t assume pre-tokenized input. Instead, it treats the **entire text as a raw stream of characters**, including whitespace.

> **"Hello world"** â†’ Could become `["_Hello", "_world"]`, where `_` represents space.

### ğŸ” Key Features:

* Works for languages with or without spaces (like Chinese or Japanese)
    
* Can use BPE or Unigram LM
    
* No need for external preprocessing
    

### ğŸ’¡ SentencePiece + Unigram Language Model

This model:

* Builds a large vocabulary of candidate subwords
    
* Uses **likelihood-based pruning** to select best tokens
    
* Allows **sampling of tokenizations** â†’ great for data augmentation
    

---

## ğŸ§  Summary Table

| Tokenizer | Handles Unknowns | Language Agnostic | Vocabulary Size | Robustness |
| --- | --- | --- | --- | --- |
| Word-Based | âŒ | âŒ | ğŸ”º Huge | ğŸ”» Low |
| Char-Based | âœ… | âœ… | ğŸ”» Small | âŒ (semantics lost) |
| BPE | âœ… | âœ… | âš–ï¸ Controlled | âœ… |
| WordPiece | âœ… | âœ… | âš–ï¸ Controlled | âœ…âœ… |
| SentencePiece | âœ…âœ… | âœ…âœ… | âš–ï¸ Controlled | âœ…âœ…âœ… |

---

---

## ğŸ§™ Understanding Special Tokens in NLP

In modern NLP modelsâ€”especially those based on Transformers like **BERT**, **GPT**, **T5**, etc.â€”youâ€™ll often encounter **special tokens**. These tokens arenâ€™t part of natural language, but are added to help the model **understand context, sequence boundaries, and task-specific information**.

Letâ€™s go through the most common special tokens:

---

### ğŸ”¸ `<PAD>` â€“ Padding Token

When batching sequences for training, not all sentences are of equal length. To ensure uniform input dimensions, we **pad** the shorter sequences with a special `<PAD>` token.

```text
Original:     ["I", "like", "pizza"]
Padded:       ["I", "like", "pizza", "<PAD>", "<PAD>"]
```

* **Used for:** Sequence alignment in batches
    
* **Ignored in attention mechanisms** via *attention masks*
    
* **Value in embeddings:** Usually mapped to a vector of zeros or a learned embedding
    

---

### ğŸ”¸ `<CLS>` â€“ Classification Token

This token is added **at the beginning of a sentence** in models like BERT.

```text
Sentence: "Transformers are powerful."
Tokenized: ["<CLS>", "Transformers", "are", "powerful", ".", "<SEP>"]
```

* The embedding corresponding to `<CLS>` is often used as the **aggregated representation** of the entire sequence.
    
* Used for tasks like **sentence classification**, **entailment**, or **sentiment analysis**.
    

> In BERT, the final hidden state of the `<CLS>` token is passed to a classifier head.

---

### ğŸ”¸ `<SEP>` â€“ Separator Token

This token is used to **separate multiple sentences** or segments within a single input.

```text
Input: ["<CLS>", "Sentence A", "<SEP>", "Sentence B", "<SEP>"]
```

* Used in tasks like:
    
    * **Next Sentence Prediction**
        
    * **Question-Answering** (where question and context are separated)
        
* Helps the model distinguish between segments
    

---

### ğŸ”¸ `<MASK>` â€“ Masking Token

Specific to **masked language modeling**, as used in **BERT**. This token hides a word in the input so the model learns to predict it.

```text
Input: "The sky is <MASK>."
```

* Trains the model to **predict missing or corrupted tokens**
    
* Encourages deeper contextual understanding
    

---

### ğŸ”¸ `<UNK>` â€“ Unknown Token

When a tokenizer encounters a word that isn't in its vocabulary and **canâ€™t be broken down into known subwords**, it assigns `<UNK>`.

* Appears in **word-level tokenizers** or poorly trained subword tokenizers
    
* Subword tokenization methods like **BPE** or **WordPiece** aim to reduce `<UNK>` usage
    

---

### ğŸ”¸ `<BOS>` / `<EOS>` â€“ Beginning/End of Sequence Tokens

These are used in **sequence generation** tasks like translation, summarization, or text generation.

* `<BOS>` = Begin Of Sequence
    
* `<EOS>` = End Of Sequence
    

```text
Input: ["<BOS>", "Hello", "world", "<EOS>"]
```

* In models like **GPT**, generation stops when `<EOS>` is predicted.
    
* In **seq2seq** models (like T5), these help mark input/output boundaries.
    

---

### ğŸ§  Summary Table: Special Tokens

| Token | Meaning | Use Case |
| --- | --- | --- |
| `<PAD>` | Padding token | Batch alignment, ignored by attention |
| `<CLS>` | Classification token | Sentence-level tasks in BERT |
| `<SEP>` | Separator token | Sentence pair tasks, QA |
| `<MASK>` | Masking token | Masked language modeling |
| `<UNK>` | Unknown token | Out-of-vocab handling |
| `<BOS>` | Begin of sequence | Text generation, decoding start |
| `<EOS>` | End of sequence | Text generation, decoding end |

---

### ğŸ’¡ Pro Tip:

When using pre-trained models from Hugging Face Transformers or TensorFlow Hub, **tokenizers automatically handle** these special tokens for you. But when you build custom models or train from scratch, you need to **define and manage** them carefully.

---

## ğŸ§ª Final Thoughts

Tokenization might seem like a simple preprocessing stepâ€”but it's actually **fundamental to a model's performance**. Whether you're building a chatbot or a translation system, understanding the nuances of tokenization will give your models the edge they need.

Choose your tokenizer wisely:

* Want speed and control? Go with **BPE**.
    
* Want precision for language models? **WordPiece** is your friend.
    
* Want flexibility and multilingual support? **SentencePiece** is your hero.